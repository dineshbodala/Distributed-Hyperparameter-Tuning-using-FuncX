# Distributed Hyperparameter Tuning (DHPT) with FuncX

## Overview

This repository contains the code for  Distributed hyperparameter tuning for machine learning models using  with FuncX.
## Key Features

- **Distributed Hyperparameter Tuning:** DHPT streamlines the hyperparameter tuning process by distributing tasks across computing endpoints, significantly reducing tuning time.
- **FuncX Integration:** Leveraging the FuncX framework, DHPT enables seamless distribution of hyperparameter tuning tasks, automating exploration of hyperparameter spaces.
- **Scalability:** The framework is designed to scale with the number of available computing endpoints, providing flexibility in handling varying workloads.
- **Resource Optimization:** DHPT aims to optimize resource utilization, ensuring efficient execution of hyperparameter tuning tasks on distributed computing resources.
- **Compatibility:** The framework is compatible with a variety of machine learning models, providing a versatile solution for different use cases.

## Getting Started

### Prerequisites

- Python 3.7 or higher
- FuncX framework installed (refer to [FuncX documentation](https://funcx.org/)).

### Installation

Clone the repository:

```bash
git clone https://github.com/yourusername/DHPT-FuncX.git

```

### Usage

1. Configure FuncX endpoints: Set up FuncX endpoints on the desired computing resources for distributed execution.

2. Run DHPT: Execute the DHPT framework by running the main script:

```bash
python dhpt.py
```

Follow the prompts to initiate the distributed hyperparameter tuning process.


